%%% -*-LaTeX-*-
%%% applicationsmetropolis.tex.orig
%%% Prettyprinted by texpretty lex version 0.02 [21-May-2001]
%%% on Tue Jan  5 10:08:17 2021
%%% for Steve Dunbar (sdunbar@family-desktop)

\documentclass[12pt]{article}

\input{../../../../etc/macros} %\input{../../../../etc/mzlatex_macros}
\input{../../../../etc/pdf_macros}

% Define a new command so Boltzmann constant is not confused.
\newcommand{\kT}{{\text k}_{\text{Boltz}}T}

\bibliographystyle{plain}

\begin{document}

\myheader \mytitle

\hr

\sectiontitle{Applications of the Metropolis Algorithm}

\hr

\usefirefox

\hr

% \visual{Study Tip}{../../../../CommonInformation/Lessons/studytip.png}
% \section*{Study Tip}

% \hr

\visual{Rating}{../../../../CommonInformation/Lessons/rating.png}
\section*{Rating} %one of
% Everyone: contains no mathematics.
% Student: contains scenes of mild algebra or calculus that may require guidance.
% Mathematically Mature: may contain mathematics beyond calculus with proofs.
Mathematicians Only:  prolonged scenes of intense rigor.

\hr

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Starter Question} For the joint probability density \(
f(x,y) \) for random variables \( X \) and \( Y \), what are the
marginal probability density functions for \( X \) and \( Y \)
separately?  What are the conditional probability density functions \( f
(x \given y) \) and \( f(y \given x) \)\,?

\hr

\visual{Key Concepts}{../../../../CommonInformation/Lessons/keyconcepts.png}
\section*{Key Concepts}

\begin{enumerate}
    \item
        For the Ising model the Metropolis algorithm takes a random walk
        through the configuration space, visiting more frequently
        occurring spin configurations more often.  Instead of choosing
        configurations randomly, then weighting them with \( \EulerE^{-E/\kT}
        \), instead choose configurations with probability \( \EulerE^{-E/\kT}
        \) and weight them evenly.
    \item
        The Gibbs sampler is an algorithm for generating random
        variables from a marginal distribution indirectly, without
        having to calculate the density.
    \item
        Bayesian hierarchical models naturally describe the connections
        between data, observed parameters and other unobserved
        parameters, sometimes called \emph{latent variables}.  Suppose \(
        f(x, y_1, y_2, \dots, y_p) \) is a probability distribution in
        which the variables represent parameters of a statistical model.
        The goal is to get point and interval estimates for these
        parameters.  This fits into the Gibbs sampling framework, using
        Bayes' rule.
    \item
        For image reconstruction, the goal is to find the configuration
        maximizing \( \Prob{\omega \given \omega^{\text{blurred}}} \),
        called the maximum a posteriori estimate.  The technique is to
        formulate a new version of the Metropolis algorithm called Gibbs
        sampling.
\end{enumerate}

\hr

\visual{Vocabulary}{../../../../CommonInformation/Lessons/vocabulary.png}
\section*{Vocabulary}
\begin{enumerate}
    \item
        The \defn{Ising model} is a simple mathematical model of a
        ferromagnet capturing the tendency for neighboring sites to
        align with each other.
    \item
        A \defn{phase transition} is a dramatic change between
        differently ordered magnetic states as a parameter
        passes through a critical value.
    \item
        A \defn{configuration} is a matrix of spins, \( \omega = (\omega_
        {1,1}, \omega_{1,2}, \dots, \omega_{N,N}) \).
    \item
        The frequency distribution of spin configurations is
        proportional to the \defn{Boltzmann weight} \(
        \operatorname{Boltz}
        (\omega) = \EulerE^{-E(\omega)/\kT} \) where \( \text{k}_{\text{Boltz}} \) is the
        Boltzmann constant.
    \item
        The \defn{magnetization} at temperature \( T \) is \( \E{M}_T \)
        is the expected value of \( M(\omega) \):
        \begin{align*}
            \E{M}_T &= \frac{1}{Z} \sum\limits_{\omega \in \Omega} M(\omega)
            \operatorname{Boltz}
            (\omega) \\
            &= \frac{1}{Z} \sum\limits_{\omega \in \Omega} M(\omega)
            \EulerE^{-E(\omega)/\kT}
        \end{align*}
        where \( Z = \sum_{\omega} \EulerE^{-E(\omega)/\kT} \) is the
        partition function, the probability normalizing factor.
    \item
        A probability distribution whose conditional probabilities
        depend on only the values in a neighborhood system is called a
        \defn{Gibbs distribution}.
    \item
        The \defn{Gibbs sampler} generates a sample from \( f(x) \) by sampling
        instead from the conditional distribution \( f(x \given y) \)
        and \( f(y \given x) \), distributions that are often known in
        statistical models.  This is done by generating a ``Gibbs
        sequence'' of random variables
        \[
            Y'_0, X'_0, Y'_1, X'_1, Y'_2, X'_2, \dots, Y'_k, X'_k.
        \] The initial value \( Y'_0 = y'_0 \) is specified and the rest
        of the sequence is obtained iteratively by alternately
        generating values from
        \begin{align*}
            X'_j &\sim f(x \given Y'_j = y'_j) \\
            Y'_{j+1} &\sim f(y \given X'_j = x'_j).  \\
        \end{align*}
        Refer to the generation of the sequence as \defn{Gibbs sampling}.
    \item
        \defn{Bayesian hierarchical models} naturally describe the
        connections between data, observed parameters and other
        unobserved parameters, sometimes called \emph{latent variables}.
\end{enumerate}

\hr

\visual{Mathematical Ideas}{../../../../CommonInformation/Lessons/mathematicalideas.png}
\section*{Mathematical Ideas}
\subsection*{The Ising Model}

In the physics of magnetism, the Curie temperature \( T_c \)%
\index{Curie temperature}
is the temperature above which certain materials lose their permanent
magnetic properties.  The Curie temperature is named after Pierre Curie,
who showed that magnetism disappears above a critical temperature. Above
\( T_c \) thermal energy disrupts the tendency toward magnetic dipole
alignment, and the substance acts as a paramagnetic one.  The \defn{Ising
model}%
\index{Ising model}
is a simple mathematical model of a ferromagnet capturing the tendency
for neighboring sites to align with each other or with an external
magnetic field.  The Ising model exhibits a \defn{phase transition},%
\index{phase transition}
a dramatic change between differently ordered magnetic states
as a parameter passes through a critical value.  The
phase change is in the \emph{magnetization},%
\index{magnetization}
the measure of the property of being magnetic.  The goal of the model is
to find \( T_c \).  Interestingly, the one-dimensional Ising model does
not have a phase transition, first proven by Ernst Ising in 1925.  A
1944 result of Lars Onsager gives the exact value for the critical
temperature in the two-dimensional Ising model:
\[
    \frac{\kT_c}{J} = \frac{2}{\ln(1 + \sqrt{2})} \approx 2.269
\] where \( \text{k}_{\text{Boltz}} \) is the Boltzmann constant from statistical mechanics
and \( J \) is a proportionality constant for the strength of alignment.

The Ising model uses a bounded planar square lattice with \( N^2 \)
sites.  An \( N \times N \) matrix represents the square lattice, the
first row at the top, and the \( N \)th row at the bottom, the first
column at the left, the \( N \)th column at the right.  At each lattice
site \( (i,j) \), there is a \emph{spin} represented by \( \omega_{i,j}
= \pm 1 \).  A \defn{configuration} is a matrix of spins, \( \omega = (\omega_
{1,1}, \omega_{1,2}, \dots, \omega_{N,N}) \).%
\index{configuration}
Define \( \Omega \) as the set of all possible configurations so \(
\card{\Omega} = 2^{N^2} \).  The magnetic energy (also called the
Hamiltonian) of a configuration is
\[
    E(\omega) = -J \sum\limits_{i, j=1}^{N} \sum\limits_{k=1}^4 \omega_{i,
    j} \omega_{\left\langle i,j \right\rangle[k]} - H \sum\limits_{i=1}^N
    \omega_i
\] where \( J > 0 \) is the nearest-neighbor affinity (a proportionality
constant with units of energy for the strength of the binding between
neighbors), \( H \ge 0 \) represents the external field and \( \langle
i, j \rangle \) indicates the set of \( 4 \) sites that are periodic
nearest neighbors, sites in the lattice with a horizontal or vertical
bond.  For the top row with \( i = 1 \), periodic means \( \left\langle 1, j
\right\rangle = \set{(2,j), (1, j-1), (N,j), (1, j+1)} \), interpreting
periodically if \( j=1 \) or \( j=N \).  Similarly for the bottom row,
and the leftmost and rightmost columns.  The frequency distribution of
spin configurations is proportional to the \defn{Boltzmann weight}%
\index{Boltzmann weight}
\(
\operatorname{Boltz}
(\omega) = \EulerE^{-E(\omega)/\kT} \) where \( \text{k}_{\text{Boltz}} \) is the Boltzmann
constant, so \( \kT \) has units of energy.  For completely ordered
configurations with \( \omega_{i,j} \equiv +1 \) or \( \omega_{i,j}
\equiv -1 \), \( E(\omega) = -J \cdot 4N^2 \) (low magnetization energy)
and \(
\operatorname{Boltz}
(\omega) = \EulerE^{J \cdot 4N^2/\kT} \) is large (relatively higher
frequency).  For highly disordered configurations with \( \omega_{i,j} =
\pm 1 \) distributed randomly, \( E(\omega) \approx 0 \) (higher
magnetization energy) and \(
\operatorname{Boltz}
(\omega) \approx \EulerE^{-0/\kT} \approx 1 \) is small (relatively lower
frequency).

For purposes of the example, assume there is no external field, so \(
H=0 \) and that \( J=1 \).

For a configuration \( \omega \) the net spin is
\[
    M(\omega) = \abs{\sum\limits_{i,j=1}^N \omega_{i,j}}.
\] The \defn{magnetization}%
\index{magnetization}
at temperature \( T \) is \( \E{M}_T \) is the expected value of \( M(\omega)
\):
\[
    \E{M}_T = \frac{1}{Z} \sum\limits_{\omega \in \Omega} M(\omega)
    \operatorname{Boltz}
    (\omega)
    = \frac{1}{Z} \sum\limits_{\omega \in \Omega} M(\omega) \EulerE^{-E
    (\omega)/\kT}.
\]
where \( Z = \sum_{\omega} \EulerE^{-E(\omega)/\kT} \) is the partition
function,%
\index{partition function}
the probability normalizing factor.  At high temperatures there is no
correlation between sites, the magnetization is nearly \( 0 \),
configurations are essentially uniformly distributed and \( \E{M}_T \)
is near zero.  This highly disordered configuration is not magnetic.  As
the temperature is lowered to a critical value called the Curie
temperature \( T_c \), spontaneous magnetization occurs.  The Curie
temperature is a critical temperature below which sites influence each
other at long distances.  It is impractical to directly compute the
expected magnetization since the number of configurations is too large.
Even for \( N=5 \), to directly compute the magnetization requires \( 2^
{25} \) matrices with \( 25 \) entries, over \( 838 \) million values.
% Edited to here 12/16/2020

Since direct calculation is unfeasible, the goal is to apply the
Metropolis algorithm to the Ising model to identify \( T_c \).  To do
that, run the Markov chain Monte Carlo method many times at different
temperatures to find the average magnetization as a function of
temperature to find where the phase change occurs.  The Metropolis
algorithm takes a random walk through the configuration space, visiting
more frequently occurring spin configurations more often.  Instead of
choosing configurations randomly, then weighting them with \( \EulerE^{-E/\kT}
\), instead choose configurations with probability \( \EulerE^{-E/\kT} \)
and weight them evenly.
\begin{enumerate}
    \item
        The proposed transition process between configurations is to
        pick a lattice site \( (i, j) \) uniformly at random from \( i,
        j = 1, 2, 3, \dots, N \) and set \( \omega'_{(i,j)} = -\omega_{(i,j)}
        \) with probability \( 1/2 \).  That is, flip the spin \( \omega_
        {(i,j)} \) to its opposite value, otherwise with probability \(
        1/2 \) keep it at the current value.  (The probability of no
        flip of sign guarantees that the Markov chain is aperiodic.)
        Keep \( \omega'_{(i', j')}
        = \omega_{(i',j')} \) for all \( (i', j') \ne (i, j) \), that
        is, only the one selected site \( \omega_{(i,j)} \) is affected.
        Verifying the Markov chain of configuration changes is
        irreducible, aperiodic and symmetric is routine.
    \item
        The important quantity is the change in energy
        \[
            \Delta E = E(\omega') - E(\omega) = -2J (\omega'_{(i,j)} -
            \omega_{(i,j)}) \sum\limits_{k=1}^4\omega_{\left\langle i,j
            \right\rangle[k]}
        \] where the sum is only over the four nearest neighbors of the \(
        i \)th site.  The computational cost of updating a site is both
        small and independent of the size of the lattice.
    \item
        Make a random choice to take the new configuration or keep the
        previous one, weighted by the relative likelihoods of the new
        configuration versus the old one.  Using the Boltzmann
        distribution, the likelihood ratio is \( \EulerE^{\Delta E/\kT} \).
\end{enumerate}

The scripts below illustrate the two approaches.  The script \texttt
{isingDist.R} calculates the entire configuration space, the individual
energy of each configuration and the partition function. However it is
only practical for \( N = 2, 3, 4 \), not refined enough to demonstrate
the phase transition.  The second script \texttt{isingMCMC.R}
illustrates the Monte Carlo Markov chain approach.  The script directly
implements the method above and is fast but is not general.  For a
general approach, see the R package \texttt{IsingSampler}
\cite{epskamp20} and for another implementation in Python, see
\cite{schlusser18}.  The survey article by Diaconis and Saloff
\cite{DIACONIS199820} presents estimates on rates of convergence
indicating slow convergence for the Ising model Metropolis algorithm
considered here.

\subsection*{Gibbs Sampling}

The \defn{Gibbs sampler} is an algorithm for generating random variables from a
marginal distribution indirectly, without having to calculate the
density. This subsection illustrates the algorithm by exploring simple
cases.  In such cases, Gibbs sampling is based only on elementary
properties of Markov chains.

Given a joint density \( f(x, y_1, \dots, y_p) \),%
\index{joint density}
the goal is to find characteristics of the marginal density%
\index{marginal density}
\[
    f_{X}(x) = \int \dots \int f(x, y_1, \dots, y_p) \df{y_1} \dots \df{y_p},
\] such as the mean or variance. Often the integrations are extremely
difficult to do, either analytically or numerically.  In such cases the
Gibbs sampler provides an alternative method for obtaining \( f_X(x) \).

Gibbs sampling%
\index{Gibbs sampling}
effectively generates a sample \( X_1, \dots, X_m \sim f(x) \) without
requiring \( f(x) \).  By simulating a large enough sample, the mean,
variance, or any other characteristic of \( f(x) \) can be calculated to
the desired degree of accuracy.

First consider the two-variable case.  Starting with a pair of random
variables \( (X, Y) \), the Gibbs sampler generates a sample from \( f(x)
\) by sampling instead from the conditional distribution \( f(x \given
y) \) and \( f(y \given x) \), distributions often already known in
statistical models.  This is done by generating a \emph{Gibbs sequence}%
\index{Gibbs sequence}
of random variables
\begin{equation}
    Y'_0, X'_0, Y'_1, X'_1, Y'_2, X'_2, \dots, Y'_k, X'_k.%
    \label{eq:applicationsmetropolis:gibbsseq}
\end{equation}
The initial value \( Y'_0 = y'_0 \) is specified and the rest of the
sequence is obtained iteratively by alternately generating values from
\begin{align*}
    X'_j &\sim f(x \given Y'_j = y'_j) \\
    Y'_{j+1} &\sim f(y \given X'_j = x'_j).  \\
\end{align*}
The generation of the sequence is \defn{Gibbs sampling}.%
\index{Gibbs sampling}
Under reasonable general conditions, the distribution of \( X'_k \)
converges to \( f_X(x) \), the true marginal of \( X \), as \( k \to
\infty \).  Thus for \( k \) large enough, the final observation \( X'_k
\) is effectively a sample point from \( f_X(x) \).

Now develop Gibbs sampling in detail for the simple case of a \( 2
\times 2 \) table with multinomial sampling. Suppose \( X \) and \( Y \)
are each marginally Bernoulli random variables with joint distribution
\[
    \bordermatrix{
        & 0 & 1 \cr
        0 & p_1 & p_2 \cr
        1 & p_3 & p_4}
\] or in terms of the joint probability distribution
\[
    \begin{pmatrix}
        f_{X,Y}(0,0) & f_{X,Y}(1,0) \\
        f_{X,Y}(0,1) & f_{X,Y}(1,1)
    \end{pmatrix}
    =
    \begin{pmatrix}
        p_1 & p_2 \\
        p_3 & p_4
    \end{pmatrix}
    .
\] For the distribution, the marginal distribution of \( x \) is given
by
\[
    f_X = (f_X(0), f_X(1)) = (p_1 + p_3, p_2 + p_4),
\] a Bernoulli distribution with success probability \( p_2 + p_4 \).
The conditional probabilities can be expressed in two matrices
\[
    A_{y \given x} =
    \begin{pmatrix}
        \frac{p_1}{p_1 + p_3} & \frac{p_3}{p_1 + p_3} \\
        \frac{p_2}{p_2 + p_4} & \frac{p_4}{p_2 + p_4}
    \end{pmatrix}
\] and
\[
    A_{x \given y} =
    \begin{pmatrix}
        \frac{p_1}{p_1 + p_2} & \frac{p_2}{p_1 + p_2} \\
        \frac{p_3}{p_3 + p_4} & \frac{p_4}{p_3 + p_4}
    \end{pmatrix}
\] where \( A_{y \given x} \) has the conditional probabilities of \( Y \)
given \( X \) and \( A_{x \given y} \) has the conditional probabilities
of \( X \) given \( Y \).

As an example, to generate the marginal distribution of \( X \) uses the
\( X' \) sequence from \eqref{eq:applicationsmetropolis:gibbsseq}.
From \( X'_0 \) to \( X'_1 \) goes through \( Y'_0 \), so the iteration
sequence is \( X'_0 \to Y'_1 \to X'_1 \) and \( X'_0 \to X'_1 \) is a
two-stage Markov chain, with transition probability
\[
    \Prob{X'_1 \given X'_0} = \sum\limits_y \Prob{X'_1 \given Y'_1
    = y} \times \Prob{Y'_1 = y \given X'_0}.
\] For example, to go from \( x = 1 \) to \( x = 0 \) takes the dot
product of the second row of \( A_{y \given x} \) with the first column
of \( A_{x \given y} \).  Generally, \( \Prob{X'_1 \given X'_0} \) is
the right matrix multiplication of \( A_{y \given x} \) by \( A_{x
\given y} \), so
\[
    A_{x \given x} = A_{ y \given x } A_{x \given y}
\] is the transition probability matrix for the \( X' \) sequence. The
matrix that gives \( \Prob{X'_k = x_k \given X'_0 = x_0} \) is \( (A_{x
\given x})^k \).  Letting \( f_k = (f_k(0), f_k(1)) \) denote the
marginal probability distribution of \( X'_k \) then \( f_k = f_0 A^k_{x
\given x} = f_0 (A^{k-1}_{x \given x}) A_{x \given x} = f_{k-1} A_{x
\given x} \). By the Fundamental Theorem for Markov Chains, \( f_k \to f
\) as \( k \to \infty \) with stationary distribution \( f \) satisfying
\( f A_{x \given x} = f \).  If the Gibbs sequence converges, the \( f \)
satisfying \( f A_{x \given x} = f \) must be the marginal distribution
of \( X \).  In this small example, it is straightforward to check that \(
f_X = ( p_1 + p_3, p_2 + p_4) \) satisfies \( f_X A_{x \given x} = f_X \).
So stopping the iteration scheme at a large enough value of \( k \)
gives approximately \( f_X \).  The larger the value of \( k \), the
better the approximation.  No general guidance on choosing such \( k \)
is available.  However, one possible approach is to monitor density
estimates from \( m \) independent Gibbs sequences, and choosing the
first point at which these densities agree to a satisfactory degree.

The algebra for the \( 2 \times 2 \) case immediately works for any \( n
\times m \) joint distribution of \( X \)'s and \( Y \)'s. Analogously
define the \( n \times n \) transition matrix \( A_{X \given X} \) whose
stationary distribution will be the marginal distribution of \( X \).
If either (or both) of \( X \) and \( Y \) are continuous, then the
finite dimensional arguments will not work.  However, with suitable
assumptions, all of the theory still goes through, so the Gibbs sampler
still produces a sample from the marginal distribution of \( X \). The
conditional density of \( X_1 \) given \( X_0 \) is \( f_{x_1 \given x_0}
(x_1 \given x_0) = \int f_{X_1 \given Y_1}(x_1 \given y)f_{Y_1 \given X_0}
(y \given x_0) \df{y} \). Then, step by step, write the conditional
densities of \( X'_2 \given X'_0 \), \( X'_3 \given X'_0 \), \( X'_4
\given X'_0 \), \dots.  Similar to the \( k \)-step transition matrix \(
(A_{x \given x})^k \), derive an ``continuous transition probability
matrix'' with entries satisfying the relationship
\[
    f_{X'_k \given X'_0}(x \given x_0 ) = \int f_{X'_k \given X'_{k-1}}
    (X \given t) f_{X'_{k-1} \given X'_{0}}(t \given x_0) \df{t},
\] the continuous version of the right matrix multiplication. As \( k
\to \infty \) it again follows that the stationary distribution is the
marginal density of \( X \), the density to which \( f_{X'_k \given X'_0}
\), converges.

\subsection*{Gibbs Sampling in Statistics}

Suppose \( f(x_1, x_2, \dots, x_N) \) is a probability density in
which the variables represent parameters of a statistical model.  The
goal is to get point and interval estimates for these parameters.  To
fit this into the Gibbs sampling framework, assume all the
single-variable conditional probability densities
\[
    f(x_i \given x_j, j \ne i)
\] are available, that is, are a type for which samples can be obtained
using standard algorithms.  Examples of available distributions include
the uniform, the normal, the gamma, the Poisson, and any finite
distribution.  To generate a sequence of samples, select \(
\overrightarrow{x}^0 = (x_1^0, x_2^0, \dots, x_N^0) \) arbitrarily and
then create \( \overrightarrow{x}^1 = (x_1^1, x_2^1, \dots, x_N^1) \) as
follows:
\begin{enumerate}
    \item
        Generate a sample \( x_1^1 \) from \( f(x_1 \given x_2^0, x_3^0,
        \dots, x_N^0) \).
    \item
        Generate a sample \( x_2^1 \) from \( f(x_2 \given x_1^1, x_3^0,x_4^
        {0} \dots, x_N^0) \).
    \item
        Generate a sample \( x_3^1 \) from \( f(x_3 \given x_1^1, x_2^1,
        x_4^0, \dots, x_N^0) \).
    \item
        Continue in the same way through \( i = 4, \dots, N-1 \) \dots
    \item[N.]
        Generate a sample \( x_N^1 \) from \( f(x_N \given x_1^1, x_2^1,
        \dots, x_{N-1}^1) \).
\end{enumerate}
One cycle, like a raster scan of an image, produces a new value \(
\overrightarrow{x}^1 \).  Repeating this process \( M \) times
produces the sequence
\[
    \overrightarrow{x}^0, \overrightarrow{x}^1, \overrightarrow{x}^2,
    \dots, \overrightarrow{x}^M
\] approximating a sample from the joint probability density \(
f(x_1, x_2, \dots, x_N) \).

Using this sample, almost any property of the probability distribution
can be investigated.  For example, focusing on only the first component
of each \( \overrightarrow{x}^k \) produces a sample
\[
    x_1^0, x_1^1, x_1^2,\dots, x_1^M
\] from the marginal probability distribution of the first component,
formally given by the integral
\[
    f(x_1) = \int_{x_2}\cdots \int_{x_N} f(x_1, x_2, \dots, x_N) \df{x_N}
    \dots \df{x_2}.
\] In this way, Gibbs sampling can be thought of as a multi-dimensional
numerical integration algorithm.  The expected value of the first
component \( x_1 \),
\[
    \E{x_1} = \int_{x_1} x_1 f(x_1) \df{x_1}
\] is estimated by the mean of the sample \( x_1^0, x_1^1, x_1^2,\dots,
x_1^M \).  A \( 95\% \) confidence interval for \( x_1 \) can be taken
directly from the sample.

Apply Gibbs sampling  to a \defn{Bayesian hierarchical model}.%
\index{Bayesian hierarchical model}
Bayesian hierarchical models naturally describe the connections between
data, observed parameters and other unobserved parameters 
called \emph{latent variables}.  A simple three-level hierarchical model
uses Bayes' rule to bind together data, \( X \), a parameter to be
estimated, \( \lambda \), and an additional hyper-parameter, \( \beta \).
Both \( \lambda \) and \( \beta \) can be vectors.  These are connected
in the following way:
\begin{enumerate}
    \item
        At the first level, \( X \) is described by its likelihood
        function \( f(X \given \lambda) \), the probability of observing
        \( X \) conditioned on \( \lambda \).
    \item
        At the next level, \( \lambda \) is modeled by a probability
        density function, \( g(\lambda \given \beta) \), conditioned on
        the parameter \( \beta \).
    \item
        At the third level, the hyper-parameter \( \beta \) is modeled
        with another density function \( h(\beta) \).  The choice of \(
        h(\beta) \) reflects the modeler's prior beliefs about the
        likely values of \( \beta \).
\end{enumerate}
The three density functions are combined with Bayes' rule, producing a
probability density function for \( \lambda \) and \( \beta \)
conditioned on the data \( X \)
\[
    F(\lambda, \beta | X) \propto f(X \given \lambda) g(\lambda \given
    \beta) h(\beta).
\] The constant of proportionality is the reciprocal of
\[
    \int_{\lambda} \int_{\beta} f(X \given \lambda) g(\lambda \given
    \beta) h(\beta) \df{\beta} \df{\lambda}
\] which is independent of the parameters \( \lambda \) and \( \beta \),
though dependent on the data \( X \).  The integrals, (or sums, in the
case of discrete distributions) are over all values \( \lambda \) and \(
\beta \).  In most cases, the integral or sum is impossible to evaluate.
However, as before, with the Metropolis-Hastings algorithm, this
expression is not necessary.

As a specific example, consider a model of water pump failure rates. The
data, \( X \), are given by pairs \( (s_i, t_i) \) for \( i = 1,2, \dots,
10 \).  Each pair represents failure information for an individual pump.
For each pump, assume the number of failures \( s_i \) in time \( t_i \)
is given by a \(
\operatorname{Poisson}
(\lambda_i t_i) \) distribution,
\[
    f_i(s_i \given \lambda_i) = \frac{(\lambda_i t_i)^{s_i} \EulerE^{-\lambda_i
    t_i}}{s_i!}, \quad i = 1,2, \dots, 10.
\]

Assuming the failures occur independently, the product gives the
likelihood function for \( \overrightarrow{\lambda} = (\lambda_1,
\lambda_2, \dots, \lambda_{10}) \):
\[
    f(X \given \overrightarrow{\lambda}) = \prod\limits_{i=1}^{10} \frac
    {(\lambda_i t_i)^{s_i} \EulerE^{-\lambda_i t_i}}{s_i!}.
\] The traditional frequentist approach is to use \( \bar{\lambda_i} = S_i/t_i
\) as the point estimate of \( \lambda_i \) for \( i = 1,2, \dots, 10 \).
The Bayesian approach is to assume that the individual \( \lambda_i \)'s
are linked together by a common distribution.  A natural choice is a
gamma distribution with parameters \( \alpha \) and \( \beta \), so that
the density for the \( i \)th parameter is
\[
    g_i(\lambda_i \given \alpha, \beta) =\prod\limits_{i=1}^{10} \frac{\lambda_i^
    {\alpha-1} \EulerE^{-\lambda_i/\beta}}{\beta^{\alpha} \Gamma(\alpha)}.
\] The remaining hyper-parameter \( \beta \) is described by an inverse
gamma distribution with parameters \( \gamma \) and \( \delta \), so
that
\[
    h(\beta) = \frac{\delta^{\gamma} \EulerE^{-\delta/\beta}}{\beta^{\gamma+1}
    \Gamma(\gamma)}.
\] The parameters \( \gamma \) and \( \delta \) are selected to make the
top-level inverse gamma convey as little prior information as possible
about the parameters.  An extreme case of a non-informative distribution
is the uniform distribution on the parameter space.

The resulting posterior joint density for the parameters \( \lambda_1,
 \dots, \lambda_{10} \) along with the scale parameter \(
\beta \) is
\[
    F(\lambda_1, \dots, \lambda_{10}, \beta \given X) \propto
    \left[ \prod_{i=1}^{10} \frac{(\lambda_i t_i)^{s_i} \EulerE^{-\lambda_i
    t_i}}{s_i!} \right] \left[ \prod_{i=1}^{10} \frac{\lambda_i^{\alpha-1}
    \EulerE^{-\lambda_i/\beta}}{\beta^{\alpha} \Gamma(\alpha)} \right]
    \left[ \frac{\delta^{\gamma} \EulerE^{-\delta/\beta}}{\beta^{\gamma+1}
    \Gamma(\gamma)} \right].
\] For \( i = 1,2, \dots, 10 \), the density for \( \lambda_i \)
conditioned on the other parameters is proportional to
\[
    \lambda_i^{s_i + \alpha -1} \EulerE^{-\lambda_i (t_i+1/\beta)}.
\] The constant of proportionality is obtained by absorbing all factors
independent of \( \lambda_i \).  The form of the density for \( \lambda_i
\) shows that \( \Prob{\lambda_i \given \lambda_j, j \ne i, X, \beta} \)
is a gamma distribution with parameters \( s_i + \alpha -1 \) and \( 1/(t_i
+ 1/\beta) \).  Since the gamma distribution is available, Gibbs
sampling can be applied at this step.  The density for \( \beta \),
conditioned on the other parameters, is proportional to
\[
    \frac{\EulerE^{\sum_{i=1}^{10}\lambda_i + \delta}\beta}{\beta^{10\alpha+\gamma+1}}
\] showing that \( \Prob{\beta \given \lambda_1, \lambda_2, \dots,
\lambda_{10}, X} \) is an inverse gamma distribution with parameters \(
g + 10 \alpha \) and \( \sum\limits_{i=1}^{10} \lambda_i+\delta \). This
too is an available distribution.

This model is an example of a conjugate hierarchical model,%
\index{conjugate hierarchical model}
that is, one whose intermediate distributions in this case, those for
the \( \lambda_i \) and \( \beta \) are similar to the original
distributions in the hierarchical model.  This fits with the Gibbs
sampling requirement that these distributions be available.

\subsection*{Gibbs Sampling for Digital Images}

A simple model of a digital image consists of pixel elements arranged
on a rectangular lattice with \( N \) sites.  Each pixel takes a value
from a set \( S = \set{1,2, \dots, K} \) of levels, such as grayscale
or color levels.  For simplicity and analogy to the Ising model, take
a black and white image with pixel levels \( -1, +1 \).An image is a
configuration \( \omega \in \Omega \) with an assignment of a level to
each of the \( N \) sites.  Even modestly sized images result in
immensely large configuration spaces, for a \( 100 \times 100 \)
binary image, \( \card{\Omega} = 2^{10000} \).

Consider a model for image degradation with additive noise, modeled by \(
N \) independent identically distributed random variables \(
\mathcal{N} = \set{\eta_1, \eta_2, \dots \eta_N} \).  Specifically, take
noise with the \( \eta_i \) normally distributed with mean \( 0 \) and
variance \( \sigma^2 \), that is \( \eta_i \sim N(0, \sigma^2) \).
Letting \( \omega^{\text{blurred}} \) indicate the degraded or blurred
image, \( \omega^{\text{blurred}} = \omega + \mathcal{N} \).  Since the
values of \( \omega^{\text{blurred}} \) are real numbers, the resulting
image is determined by rounding each value to the nearest value in \( S \).

The relationship between the original image and the degraded version is
probabilistic, given any image \( \omega \), there is some probability a
particular \( \omega^{\text{blurred}} \) is the degraded version of \(
\omega \).  Image reconstruction looks at the problem the other way
around; given \( \omega^{\text{blurred}} \), there is some probability \(
\omega \) is the original image.  This leads to an application of Bayes'
Rule.  The \emph{posterior distribution}%
\index{posterior distribution}
for \( \omega \) conditioned on \( \omega^{\text{blurred}} \) is
\[
    \Prob{\omega \given \omega^{\text{blurred}}} = \frac{\Prob{\omega^{\text
    {blurred}} \given \omega} \Prob{\omega}} {\Prob{\omega^{\text{blurred}}}}.
\] The goal is to find the configuration maximizing \( \Prob{\omega
\given \omega^{\text{blurred}}} \), called the \emph{maximum a
posteriori estimate}.%
\index{maximum a
  posteriori estimate}
The technique is to formulate a new version of the Metropolis algorithm
with Gibbs sampling.

By the Law of Total Probability the denominator is
\[
    \Prob{ \omega^{\text{blurred}}} = \int\limits_{\omega
    \in \Omega} \Prob{\omega^{\text{blurred}} \given \omega} \Prob{\omega}
    \df{\omega}.
\] This integral (or sum) is over all \( \omega \in \Omega \) and does
not depend on \( \omega \).  This is reminiscent of the partition
function and recalling the Metropolis algorithm, just ignore it.

The likelihood function \( \Prob{\omega^{\text{blurred}} \given \omega} \)
is%
\index{likelihood function}
\[
    \Prob{\omega^{\text{blurred}} \given \omega} \propto \prod_{i=1}^N
    \EulerE^{-\frac{ (\omega_i^{\text{blurred}} - \omega_i)^2}{2 \sigma^2}}
    = \EulerE^{ -\frac{1}{2\sigma^2} \sum\limits_{i=1}^N (\omega_i^{\text
    {blurred}} - \omega_i)^2}
\] where any constant of proportionality will be absorbed into the
denominator above.

An image has patterns, i.e.\ contiguous regions of similar
pixel values.  On the other hand, if neighboring values are
uncorrelated, the result is the visual equivalent of noise.  This is
reminiscent of the Ising model, where after magnetization the lattice
has high-degree long-range correlation between pixels, that is,
image-like features.

This suggests using the Boltzmann probability distribution with the
Ising potential energy function as the prior distribution on images, that is
\[
    \Prob{\omega} \propto \EulerE^{-E_{\text{ising}}(\omega)/\kT}
  \] where
\[
    E_{\text{ising}}(\omega) = -J \sum\limits_{i=1}^{N} \sum\limits_{k=1}^4 \omega_{i,
    j} \omega_{\langle i\rangle[k]} 
\] is the nearest neighbor affinity.
To retain the idea of correlated pixel values, let \( \kT/J = 1 < T_c \),
the critical temperature below which a phase transition occurs.

Putting all the parts together, the posterior distribution, \( \Prob{\omega
\given \omega^{\text{blurred}}} \) is
\begin{align*}
    \Prob{\omega \given \omega^{\text{blurred}}} &\propto \EulerE^{-\frac
    {1}{2\sigma^2 } \sum\limits_{i=1}^N(\omega_i^{\text{blurred}} -
    \omega_i)^{2}} \EulerE^{-E_{\text{ising}}} \\
    &\propto \EulerE^{-\left[ \frac{1}{2\sigma^2 } \sum\limits_{i=1}^N(\omega_i^
    {\text{blurred}} - \omega_i)^{2} + E_{\text{ising}}  \right] }.
\end{align*}
Viewing this from a
statistical mechanics perspective leads to an analog of an energy
function
\begin{align*}
    E_{\text{image}}(\omega \given \omega^{\text{blurred}}) &= \frac{1}{2\sigma^2
      } \sum\limits_{i=1}^N(\omega_i^{\text{blurred}} - \omega_i)^{2} +
      E_{\text{ising}} \\
    &= \frac{1}{2\sigma^2 } \sum\limits_{i=1}^N(\omega_i^{\text{blurred}}
    + \omega_i)^{2} - \sum\limits_{\langle i\rangle[k]} \omega_i\omega_k
    \\
\end{align*}
where \( {\langle i\rangle} \) indicates the set of nearest neighbor
sites for site \( i \).

Finding the most probable original image \( \omega \) given \( \omega^{\text
{blurred}} \) is thus equivalent to minimizing \( E_{\text{image}}(\omega
\given \omega^{\text{blurred}}) \).  The first term is a positive
potential energy
penalty for
straying too far from the data \( \omega^{\text{blurred}} \) while the
second term represents a negative potential energy reflecting
the desire to align neighboring pixel values
making them conform to the prior notion of a generic image.  The optimal
solution balances between these two conflicting objectives.

Creating an energy function to minimize leads to an interesting
contrast.  The Ising model starts
with an objective function and interprets it as a energy function, using
this to convert it to a probability from a physical interpretation.  The
image reconstruction model starts with a probabilistic situation with a
Bayesian structure, leading to an energy function.

To implement Gibbs sampling, the
probability of \( \omega_i \) conditioned on \emph{all} the other sites depends
on \emph{only} the sites in the nearest neighborhood set.  Suppressing the
dependence on \( \omega^{\text{blurred}} \), this means
\begin{align*}
    \Prob{\omega_i \given \omega_j, j \ne i }  &= \Prob
    { \omega_i \given \omega_j, j \in \langle i\rangle } \\
    & \propto \EulerE^{-E_i(\omega_i \given \omega_j, j \in
    \langle i \rangle)}
\end{align*}
where
\[
  E_i(\omega_i \given \omega_j, j \in \langle i \rangle) =
  \frac{1}{2\sigma^2}(\omega_i^{\text{blurred}}-\omega_i)^2 -
    \sum\limits_{\langle i,\rangle[k]} \omega_i\omega_k.
\] (Continue to check signs on this
energy function.) A probability distribution whose conditional probabilities depend on
only the values in a neighborhood system is called a \defn{Gibbs
distribution}%
\index{Gibbs distribution}
and is an example of a larger notion called a \emph{Markov random field}.%
\index{Markov random field}

A standard way to implement Gibbs sampling for images is to use a
sequence of raster scans, in order by rows or columns, guaranteeing all
sites are visited many times.  At a selected site \( i \), select \(
\omega_i = k \) with probability
\[
    \Prob{\omega_i = k} \propto \EulerE^{-\frac{1}{2\sigma^2}(\omega_i^{\text
    {blurred}}-k)^2 - \sum\limits_{\langle i \rangle[j]} k \omega_j}.
\] Repeating this with many raster scans results in a sequence of images
that approximates a sample from the posterior distribution. Note the
connection to the previous simple Gibbs sampling algorithm, generating a
sample from \( f(x) \) by sampling iteratively from the conditional
distribution \( f(x, \given y) \),
except that now there are many more conditional distributions, one
for each pixel.

Gibbs sampling fits into the Hastings generalization of the Metropolis
algorithm in the following way:  In the second step of the
Metropolis-Hastings algorithm, the probabilities \( \alpha_{ij} \) are
all equal to \( 1 \).  However, the transitions are no longer
time-independent, since each depends on the site choice.  As a result,
the proof is somewhat more involved than the original proofs of
convergence given by Hastings.  Gibbs sampling will produce a sequence
representing a sample from \( \Prob{\omega \given \omega^{\text{blurred}}}
\).  The full algorithm also includes a ``temperature'' parameter \( T \)
to create a simple form of simulated annealing.

\begin{theorem}
    Assume
    \begin{enumerate}
        \item
            an image with \( N \) pixels,
        \item
            \( T_k \) is a any decreasing sequence of temperatures such
            that
            \begin{enumerate}
                \item
                    \( T_k \to 0 \) as \( k \to \infty \),
                \item
                    \( T_k \ge N \Delta/\ln(k) \) for all sufficiently
                    large \( k \) and constant \( \Delta \).
            \end{enumerate}
    \end{enumerate}
    Then starting at \( \omega^{(0)} = \omega^{\text{blurred}} \), the
    Gibbs sampling sequence \( \omega^{(k)} \) for \( k=0,1,2, \dots \)
    converges in distribution to the distribution which is uniform on
    the minimum vales of \( E_{\text{image}}(\omega) \) and \( 0 \)
    otherwise.
\end{theorem}

In other words, following a prescribed annealing schedule, Gibbs
sampling must, in theory, produce a maximum a posteriori estimate of \(
\Prob{\omega \given \omega^{\text{blurred}}} \).

Even though this result guarantees convergence to the most likely image,
the rate of convergence is slow.  Theoretically, for a \( 100 \times 100 \) lattice
with \( N = 10^4 \) pixels, using the theorem requires \( \EulerE^{20{,}000}
\) steps to go from \( T = 4 \) to \( T = 0.5 \).  In practice, it takes
about \( 300 - 1000 \) raster scans to produce acceptable results.

For a two-color ($k = -1$ or $1$) image Gibbs sampling with annealing is especially straightforward
to implement using the ideas of the section on Gibbs sampling.  At the
pixel $\omega_i$ define
\[
    E^k = \frac{1}{2 \sigma^2}( k - \omega_i^{\text{blurred}})^2 + k
    \sum\limits_{\langle i,j\rangle} \omega_j.
  \]
  Set $\omega_i = \pm 1$ with probability
  \[
    \frac{\EulerE^{-E^k/T}}{\EulerE^{-E^{-1}/\kT} + \EulerE^{-E^1/\kT}}
  \]
  
\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Ending Answer}

For notational simplicity, assume \( X \) and \( Y \) are discrete.
Then the marginal densities are \( f_X(x) = \sum\limits_y f(x,y) \) and \(
f_Y(y) = \sum_x f(x,y) \).  The conditional densities are \( f(x \given
y) = f(x,y)/\sum\limits_y f(x,y) \) and \( f(y \given x) = f(x,y)/\sum\limits_x
f(x,y) \).

\subsection*{Sources}

This section is adapted from
\cite{richey10}.  Additional ideas about the Ising model and the
associated Metropolis algorithm are adapted from
\cite{schlusser18}.  The subsection on the elementary formulation of
Gibbs sampling is from
\cite{casella92}.

\hr

\visual{Algorithms, Scripts, Simulations}{../../../../CommonInformation/Lessons/computer.png}
\section*{Algorithms, Scripts, Simulations}

\subsection*{Algorithm}

\begin{codebox}
\li  \Procname{IsingDist}
\li  \Comment Plot magnetization versus temperature for the Ising model.
\li  \Comment WARNING: Use only for $N=2,3,4$.
\li  \kw{Require:}  Affinity constant $J$
\li  \kw{Require:} Size (side length) of grid $N$
\li  \kw{Require:} Number of normalized temperatures, and list of
\zi    temperatures around $kT_c/J = 2.269$ when $J=1$
\li  \Return Plot of magnetization versus temperature
\li  \For Temperature array
\li      \Do
               Calculate magnetization over all configurations
\li  \End
\li  Plot magnetization versus temperatures
\zi  
\li  \proc{magnetization}(configurations, normalized Temperature)
\li   Utility function to compute magnetization at a temperature
\li   uses \proc{partitionFunction}, \proc{netSpin}, \proc{boltzDist}
\zi
\li  \proc{netSpin}(configurations, $N$)
\li   Utility function to compute
\li   Absolute value of sum of entries in each matrix
\zi
\li  \proc{partitionFunction}(configurations, normalized Temperature)
\li   Utility function to compute the sum of all Boltzman energies
\li   uses \proc{boltzDist}
\zi
\li  \proc{boltzDist}(configurations, normalized Temperature)
\li   Utility function to compute energies of all configurations
\li   Uses \proc{allEnergies}
\zi
\li  \proc{allEnergies}(configurations)
\li   Apply energy function across all configurations
\li   Uses \proc{energy}
\zi
\li  \proc{energy}(config, $J=1$)
\li   Computes potential energy by nearest neighbor affinity,
\li   value is negative with lowest energy $-4 N^2$.
\zi
\li  \proc{allConfigs}($N$)
\li   Utility function returns LIST of all square configs
\li   uses \proc{allBinarySeq}
\zi
\li  \proc{allBinarySeq}($N$)
\li   Utility function to make all possible combinations of $\pm 1$
\li  over $N$ places.
i\end{codebox}

\begin{codebox}
\li  \Procname{IsingMetropolis}
\li  \Comment Find magnetization for a temperature for the Ising model
\li    using Metropolis algorithm
\li  \kw{Require:} Affinity constant $J$, normalized temperature
\li  \kw{Require:} Size of grid $N$, number of steps for MCMC algorithm
\li   Create a random configuration of size $N \times N$.
\li  \For{$i$ in number of MCMC algorithm steps}
\li  \Do 
\li       Create a new configuration using Metropolis MCMC step
\li  \End
\li   Plot grayscale image of final configuration
\zi
\li  \proc{metropolisMCStep}(configuration, $N$)
\li   Pick a random site in the configuration.
\li   Flip spin at the random site, label it with prime.
\li   Find the coordinate of the (periodic) neighboring sites.
\li   Using the neighboring sites, find the change in energy due to
      the flip.
\li   Compute the Boltzman function $p$ of the energy change
\li  \If random uniform variate less than $p$
\li      \Then
          The new site value is the prime, return new configuration.
\li  \Else
          Return original configuration.
\li  \End
\li       uses \proc{randomSite}, \proc{boltzWeight}, \proc{surroundNeighbors}
\zi          
\li  \proc{surroundingNeighbors}(site)
\li   Utility function to return the coordinates of the (periodic) neighboring states.

\li  \proc{boltzWeight}(energy, normalized temperature)
\li   Utility function to return the Boltzman weight
\li   of the given energy and temperature.
\zi
\li  \proc{randomSite}($N$)
\li   Utility function to return a randomly picked site from $N \times N$
  configuration.
\zi
\li  \proc{randomConfig}(N)
\li   Utility function to return a random configuration,
\li   an $N \times N$ matrix of $\pm 1$. 
\end{codebox}

\begin{codebox}
\li  \Comment Use Gibbs sampling to restore a blurred picture
\li  \kw{Require:} Size $N$ of $N \times N$ grid, number of iterations of
    raster scan.
\li   Create a test picture with utility function  \proc{makePicture}
\li   Blur picture with utility function \proc{makeBlurredPicture}
\li   Copy blurred picture into a holding place
\li  \For Number of iterations
\li    Make a new picture with the \proc{rasterScan} function
\li   Copy new picture into holding place
\li  \End
\li   Plot the test Picture and the raster scanned picture
\zi
\li  \proc{rasterScan}(picture, blurred picture, $\sigma$)
\li   Get the number of rows and columns
\li   Do a raster scan over the picture with the utility function \proc{gibbsSample}.
\zi
\li  \proc{gibbsSample}(site, picture, blurred, $\sigma$)
\li   Get a uniform random variate $x$.
\li   Calculate $p$ with utility function \proc{probSite}
\li  \If  $x < p$
\li   Set $k = 0$
\li  \Else
\li   Set $k = 1$
\li  \End
\li   Return $k$
\zi
\li  \proc{probSite}(site, picture, blurred, $N$, $\sigma$)
\li   The first term is a penalty for
 straying too far from the data \( \omega^{\text{blurred}} \) while the
second term represents the desire to align neighboring pixel values
making them conform to the prior notion of a generic image.
\li  Return conditional probability
\li Uses utility function \proc{surroundNeighbors}

\li\proc{surroundNeighbors}(site, $N$)
\li Utility function to return (periodic) nearest neighbors.
\zi
\li\proc{makeBlurredPicture}($N$, $\sigma$)
\li Utility function to make a specked, blurred image
\li\Comment Uses utility functions \proc{makeNoise}, \proc{roundPicture}
\zi
\li\proc{roundPicture}($M$)
\li Round real number to nearest of $0$, $1$.
\zi
\li\proc{makeNoise}
\li Generate some random (Gaussian, mean $0$, standard deviation
\li$sigma$), to add to a picture.
\zi
\li\proc{makePicture}($N$)
\li Make a simple test picture with two rectangular patches.
\end{codebox}

\subsection*{Scripts}

\input{applicationsmetropolis_scripts}

\hr

\visual{Problems to Work}{../../../../CommonInformation/Lessons/solveproblems.png}
\section*{Problems to Work for Understanding}

\renewcommand{\theexerciseseries}{}
\renewcommand{\theexercise}{\arabic{exercise}}

\begin{exercise}
    Suppose \( X \) and \( Y \) are each marginally Bernoulli random
    variables with joint distribution
    \[
        \bordermatrix{
            & 0 & 1 \cr
            0 & \frac{1}{5} & \frac{1}{4} \cr
            1 & \frac{1}{3} & \frac{13}{60} }.
    \] Use Gibbs sampling to find the marginal distribution of \( X \).
\end{exercise}
\begin{solution}
    The conditional probabilities are the two matrices
    \[
        A_{y \given x} =
        \begin{pmatrix}
            \frac{12}{32} & \frac{20}{32} \\
            \frac{15}{28} & \frac{13}{28}
        \end{pmatrix}
    \] and
    \[
        A_{x \given y} =
        \begin{pmatrix}
            \frac{12}{27} & \frac{15}{27} \\
            \frac{20}{33} & \frac{13}{33}
        \end{pmatrix}
        .
    \] Then by matrix multiplication,
    \[
        A_{x \given x} =
        \begin{pmatrix}
            \frac{6}{11} & \frac{5}{11}\\
            \frac{40}{77} & \frac{37}{77}
        \end{pmatrix}
        .
    \] The stationary distribution is \( (\frac{8}{15}, \frac{7}{15}) \).
    The eigenvalues of \( A_{x \given x} \) are \( 1 \) and \( 2/77 \)
    so the Gibbs sampling converges very rapidly.  The following script
    generates an example Gibbs sequence starting from \( 1 \).
\begin{lstlisting}
p1  <- 1/5
p2  <- 1/4
p3  <- 1/3
p4  <- 13/60

AyIx  <- matrix(c(p1/(p1 + p3), p3/(p1 + p3), p2/(p2 + p4), p4/(p2 + p4)),
                nrow=2, byrow=TRUE)
AxIy  <- matrix(c(p1/(p1 + p2), p2/(p1 +p2), p3/(p3+p4), p4/(p3 + p4)),
                nrow=2, byrow=TRUE)

Xpi  <- 0                               #not needed
Ypi  <- 1                               #initial value for gibbsSeq

burnIn  <- 100
N  <- 1000
gibbsSeq  <- matrix(0, N, 2)

for (i in 1:N) {
    Xpi  <- rbinom(1,1, AxIy[2, Ypi + 1]) #recall indexing from 1
    Ypi  <- rbinom(1,1, AyIx[Xpi+1, 2])
    gibbsSeq[i, ]  <- c(Xpi, Ypi)
}

apply(gibbsSeq[(burnIn+1):N, ], 2, sum)/(N - burnIn)
\end{lstlisting}
\end{solution}

\begin{exercise}
    The joint beta-binomial distribution for random variables \( X \)
    and \( Y \) is
    \[
        f(x, y) = \frac{1}{B(\alpha, \beta)} \binom{n}{x} y^{x + \alpha
        - 1} (1-y)^{n-x+\beta-1}
    \] where \( x = 0, 1, \dots,n \) and \( 0 \le y \le 1 \) and
    \[
        B(\alpha, \beta) = \int\limits_0^1 t^{\alpha-1} (1-t)^{\beta-1}
        \df{t} = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha +
        \beta)}
    \] is the beta function. Use the Gibbs sampler to approximate the
    marginal density \( f_{X}(x) \) of \( X \) for \( n = 16 \), \(
    \alpha = 2 \) and \( \beta = 4 \).  Use \( 500 \) independent Gibbs
    samples each of length \( 10 \) to plot a histogram of the sample
    sequence.  The conditional distributions are \( f(x \given y) \sim
    \operatorname{Binomial}
    (n, y) \) and \( f(y \given x) \sim \operatorname{Beta}
    (x + \alpha, n - x + \beta) = \frac{1}{B(\alpha, \beta)} y^{x +
    \alpha-1} (1-y)^{n-x+\beta-1} \).
\end{exercise}
\begin{solution}
\begin{lstlisting}
alphaParam  <- 2
betaParam  <- 4
n  <- 16
trials  <- 500

Xpi  <- rep(8, trials)                  #not needed
Ypi  <- rep(0.5, trials)                #initial value for gibbsSeq

gibbsIter  <- 10

for (i in 1:gibbsIter) {
    Xpi  <- rbinom(trials, 16, Ypi)     #note vectorization!
    Ypi  <- rbeta(trials, Xpi + alphaParam, n - Xpi + betaParam)
}

hist(Xpi, breaks=n)
\end{lstlisting}
\end{solution}

\hr

\visual{Books}{../../../../CommonInformation/Lessons/books.png}
\section*{Reading Suggestion:}

\bibliography{../../../../CommonInformation/bibliography}

%   \begin{enumerate}
%     \item
%     \item
%     \item
%   \end{enumerate}

\hr

\visual{Links}{../../../../CommonInformation/Lessons/chainlink.png}
\section*{Outside Readings and Links:}
\begin{enumerate}
    \item
    \item
    \item
    \item
\end{enumerate}

\section*{\solutionsname}
\loadSolutions

\hr

\mydisclaim \myfooter

Last modified:  \flastmod

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
